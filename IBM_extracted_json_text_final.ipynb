{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67c039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# This script take the IBM Deepsearch parsed JSON and converts it into text.\n",
    "# We will parse the text at page level and it can be customized to fet full document text as well\n",
    "# By just changing the full_text+= (\"\\n\" + str(value)[:-2][2:] ) function\n",
    "# Also, observed that one file was not parsed properly by IBM DeepSearch since it was scanned\n",
    "# IBM deepsearch only found the picture element from it.\n",
    "# The file is GDC-submission_German-Youth-IGF\n",
    "# So implemented tesseract and pdf2image to extract the text.\n",
    "#####\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from pdf2image import convert_from_path\n",
    "#installed poppler using conda install -c conda-forge poppler\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'set path to tesseract executable'\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class DeepSearchData(BaseModel):\n",
    "    data: dict\n",
    "        \n",
    "def ocr_to_text(pdf_file_path):\n",
    "\n",
    "    pdfs = pdf_file_path\n",
    "    pages = convert_from_path(pdfs, 350, poppler_path=r'set path to poppler')\n",
    "    image_dir=\"./images\"\n",
    "\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "    \n",
    "    i = 1\n",
    "    for page in pages:\n",
    "        image_name = \"Page_\" + str(i) + \".jpg\"  \n",
    "        page.save(f\"{image_dir}/{image_name}\", \"JPEG\")\n",
    "        i = i+1\n",
    "\n",
    "    # Set the path to the directory where you want to save the text files\n",
    "    text_output_directory = unparsed_doc\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(text_output_directory, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "            # Construct the full path to the image\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "            # Perform OCR on the image\n",
    "            text = pytesseract.image_to_string(Image.open(image_path), lang='eng')\n",
    "\n",
    "            # Construct the full path to the text file\n",
    "            text_file_path = os.path.join(text_output_directory, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "\n",
    "            # Write the OCR results to the text file\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                text_file.write(text)\n",
    "    \n",
    "    shutil.rmtree(image_dir)\n",
    "\n",
    "\n",
    "def json_to_plain_text(raw_doc: dict):\n",
    "    '''\n",
    "    Converts input json extracted from IBM deepsearch to plain text.\n",
    "    param: raw_doc - json extracted form IBM deepsearch PDF parsing operation\n",
    "    '''\n",
    "    \n",
    "    pages={}\n",
    "\n",
    "    for section in raw_doc.data['main-text']:\n",
    "        d={}\n",
    "        table_items=[]\n",
    "        if \"$ref\" in section:\n",
    "            c={}\n",
    "        \n",
    "            if section[\"type\"] in [\"footnote\", \"page-footer\", \"page-header\", \"figure\", \"table\", \"equation\", \"reference\"]:\n",
    "            \n",
    "                item_number=int(section['$ref'].split('/')[2])\n",
    "                item_container=section['$ref'].split('/')[1]\n",
    "                if item_container in [\"tables\"]:\n",
    "                    for i in raw_doc.data[item_container][item_number]['cells']['data']:\n",
    "                        table_items.append(i[-1])\n",
    "                \n",
    "                    c.setdefault(item_container, []).append(table_items)\n",
    "                    pages.setdefault(page_num, []).append(c.copy())\n",
    "                \n",
    "                else:\n",
    "                    container_name=raw_doc.data[item_container][item_number]\n",
    "                    page_num=container_name['prov'][0]['page']\n",
    "                    if container_name['type'] in ['picture']:\n",
    "                        continue\n",
    "                    else:\n",
    "                        value=container_name['text']\n",
    "                        c.setdefault(item_container, []).append(value)\n",
    "                        pages.setdefault(page_num, []).append(c.copy())\n",
    "            \n",
    "        else:\n",
    "            container_name=section['type']\n",
    "        \n",
    "            page_num=section['prov'][0]['page']\n",
    "            value=section['text']\n",
    "            d.setdefault(container_name, []).append(value)\n",
    "            pages.setdefault(page_num, []).append(d)\n",
    "    \n",
    "    doc_page=[]\n",
    "    \n",
    "    for page in pages:\n",
    "        full_text=\"\"\n",
    "        for page_items in pages[page]:\n",
    "            key, value=list(page_items.items())[0]\n",
    "            full_text+= (\"\\n\" + str(value)[:-2][2:] )\n",
    "        doc_page.append(full_text)\n",
    "    return doc_page\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    result_dir = \"./results/\"\n",
    "    text_dir=\"./text/\"\n",
    "\n",
    "    if not os.path.exists(text_dir):\n",
    "        os.makedirs(text_dir)\n",
    "        \n",
    "    for file in os.listdir(result_dir):\n",
    "        if file.endswith(\".json\"):\n",
    "            filepath= os.path.join(result_dir, file)\n",
    "            raw_doc = DeepSearchData(data=json.loads(open(filepath).read()))\n",
    "            plain_text_pages=json_to_plain_text(raw_doc)\n",
    "            text_file=file.split(\".\")[0]\n",
    "            GDC_text_path=os.path.join(text_dir, text_file)\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(GDC_text_path, exist_ok=True)\n",
    "            i=0\n",
    "            for page in plain_text_pages:\n",
    "                i+=1\n",
    "                with open(os.path.join(GDC_text_path, f\"Page_{i}.txt\"), \"w\", encoding=\"utf-8\") as t:\n",
    "                    t.write(page)\n",
    "    \n",
    "    for dirs in os.listdir(text_dir):\n",
    "        if len(os.listdir(os.path.join(text_dir, dirs)))==0:\n",
    "            unparsed_doc= (os.path.join(text_dir, dirs))\n",
    "            unparsed_actual_path= f\"./data/{dirs}\" + \".pdf\"\n",
    "            ocr_to_text(unparsed_actual_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809b85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_applications",
   "language": "python",
   "name": "chat_applications"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
